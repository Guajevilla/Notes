# 面试
## RNN和LSTM的区别：

RNN没有细胞状态；LSTM通过细胞状态记忆信息。
RNN激活函数只有tanh；LSTM通过输入门、遗忘门、输出门引入sigmoid函数并结合tanh函数，添加求和操作，减少梯度消失和梯度爆炸的可能性。
RNN只能够处理短期依赖问题；LSTM既能够处理短期依赖问题，又能够处理长期依赖问题。

## C++设计模式
创建型模式，共五种：工厂方法模式、抽象工厂模式、单例模式、建造者模式、原型模式。
结构型模式，共七种：适配器模式、装饰器模式、代理模式、外观模式、桥接模式、组合模式、享元模式。
行为型模式，共十一种：策略模式、模板方法模式、观察者模式、迭代子模式、责任链模式、命令模式、备忘录模式、状态模式、访问者模式、中介者模式、解释器模式。

死锁产生原因：（1）竞争资源（2）进程推进顺序不当

对于一个分布式计算系统来说，一致性、可用性、分区容错性 这3个指标不能同时完成。

网络延迟是指在传输介质中传输所用的时间，即从报文开始进入网络到它开始离开网络之间的时间。

链接阶段可以发现被调用的函数未定义
## 构造函数与异常
1.不建议在构造函数中抛出异常；
2.构造函数抛出异常时，析构函数将不会被执行,需要手动的去释放内存
1.析构函数不应该抛出异常；
2.当析构函数中会有一些可能发生异常时，那么就必须要把这种可能发生的异常完全封装在析构函数内部，决不能让它抛出函数之外；
3. 析构函数异常相对要复杂一些，存在一种冲突状态，程序将直接崩溃：异常的被称为&ldquo;栈展开(stack unwinding)&rdquo;【备注】的过程中时，从析构函数抛出异常，C++运行时系统会处于无法决断的境遇，因此C++语言担保，当处于这一点时，会调用 terminate()来杀死进程。因此，当处理另一个异常的过程中时，不要从析构函数抛出异常, 抛出异常时，其子对象将被逆序析构

## 柔性数组
柔性数组既数组大小待定的数组，C语言中结构体的最后一个元素可以是大小未知的数组，0长度，不占大小。
作用：1.满足需要可变长度的结构体
2.解决使用数组时内存的冗余和数组的越界

最差时间分析 平均时间复杂度 稳定度 空间复杂度
冒泡排序 O(n2) O(n2) 稳定 O(1)
快速排序 O(n2) O(n*log2n) 不稳定 O(log2n)~O(n)
选择排序 O(n2) O(n2) 稳定 O(1)
二叉树排序 O(n2) O(n*log2n) 不一顶 O(n)
插入排序 O(n2) O(n2) 稳定 O(1)
堆排序 O(n*log2n) O(n*log2n) 不稳定 O(1)
希尔排序 O O 不稳定 O(1)

关于支持向量机SVM
L2正则项，作用是最大化分类间隔，使得分类器拥有更强的泛化能力
Hinge 损失函数，作用是最小化经验分类错误
分类间隔为1/||w||，||w||代表向量的模

## 深度学习训练中梯度消失的原因有哪些？有哪些解决方法？
梯度消失产生的主要原因有：一是使用了深层网络，二是采用了不合适的损失函数。

（1）目前优化神经网络的方法都是基于BP，即根据损失函数计算的误差通过梯度反向传播的方式，指导深度网络权值的更新优化。其中将误差从末层往前传递的过程需要链式法则（Chain Rule）的帮助。而链式法则是一个连乘的形式，所以当层数越深的时候，梯度将以指数形式传播。梯度消失问题一般随着网络层数的增加会变得越来越明显。在根据损失函数计算的误差通过梯度反向传播的方式对深度网络权值进行更新时，得到的梯度值接近0，也就是梯度消失。

（2）计算权值更新信息的时候需要计算前层偏导信息，因此如果激活函数选择不合适，比如使用sigmoid，梯度消失就会很明显，原因如果使用sigmoid作为损失函数，其梯度是不可能超过0.25的，这样经过链式求导之后，很容易发生梯度消失。

 

解决方法：

（1）pre-training+fine-tunning

此方法来自Hinton在2006年发表的一篇论文，Hinton为了解决梯度的问题，提出采取无监督逐层训练方法，其基本思想是每次训练一层隐节点，训练时将上一层隐节点的输出作为输入，而本层隐节点的输出作为下一层隐节点的输入，此过程就是逐层“预训练”（pre-training）；在预训练完成后，再对整个网络进行“微调”（fine-tunning）。此思想相当于是先寻找局部最优，然后整合起来寻找全局最优，此方法有一定的好处，但是目前应用的不是很多了。

（2） 选择relu等梯度大部分落在常数上的激活函数

relu函数的导数在正数部分是恒等于1的，因此在深层网络中使用relu激活函数就不会导致梯度消失的问题。

（3）batch normalization

BN就是通过对每一层的输出规范为均值和方差一致的方法，消除了权重参数放大缩小带来的影响，进而解决梯度消失的问题，或者可以理解为BN将输出从饱和区拉到了非饱和区。

（4） 残差网络的捷径（shortcut）

相比较于之前的网络结构，残差网络中有很多跨层连接结构（shortcut），这样的结构在反向传播时多了反向传播的路径，可以一定程度上解决梯度消失的问题。

（5）LSTM的“门（gate）”结构

LSTM全称是长短期记忆网络（long-short term memory networks），LSTM的结构设计可以改善RNN中的梯度消失的问题。主要原因在于LSTM内部复杂的“门”(gates)，LSTM通过它内部的“门”可以在更新的时候“记住”前几次训练的”残留记忆“。

## 过拟合？
过拟合：在机器学习模型训练或者深度学习模型训练的过程中，会出现模型在训练集上表现能力好，但是在测试集上表现欠佳，这种现象就是过拟合，常常主要原因是由于数据集中存在噪音数据或者训练样本维度太少或者训练集数量太少导致的。
解决方案：
增强训练样本集；
增加样本集的维度；
如果模型复杂度太高，和训练样本集的数量级不匹配，此时需要降低模型复杂度；
正则化，尽可能减少参数；
添加Dropout