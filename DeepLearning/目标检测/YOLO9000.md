# YOLOv2&YOLO9000
## Better
### Batch Normalization
BN改善收敛性，并有助于regularization，加入BN后甚至可以去除dropout层而不过拟合。
### 高分辨率分类器
在v1中，YOLO是在ImageNet上利用224尺寸预训练的，这使得网络必须在切换到学习目标检测时学习调整到新的输入分辨率。所以YOLOv2中分类网络的预训练也是在高分辨率448尺寸上训练。
### Convolutional With Anchor Boxes
YOLO是直接用卷积网络预测bbox坐标而Faster R-CNN是预测anchor的偏移和置信度，这样网络更容易学习。
1. 从YOLO中移除全连接层，并使用anchor来预测bbox。
2. 消除了一个池化层，使网络卷积层输出具有更高的分辨率，最终网络下采样stride=32。
3. 将输入图像从448尺寸变为416尺寸。这样feature map大小为13\*13，原因是图像中的大目标往往在图像中心，为了不得到多个重叠的bbox，想要正中间是正好一个网格而不是偶数的4个网格。
4. 解耦类预测机制与空间位置，改为对每一个anchor预测类别和对象性(objectness)。objectness指的是ground truth与bbox的$\textrm{IOU}_{\textrm{pred}}^{\textrm{truth}}$，类预测的是$\Pr(\textrm{Class}_i | \textrm{Object})$。

使用anchor使得精度（mAP）略微下降，但Recall（TP/(TP+FN)=TP/P）相对升高了。
![](_v_images/20190521162158688_12299.png =660x)
### 维度聚类Dimension Clusters
使用anchor的第一个问题：anchor的尺寸位置是手动选择的，虽然网络会学习调整bbox尺寸位置，但如果anchor的先验维度更好会更易于检测。

在训练集边界框上运行k-means聚类，自动找到好的先验尺寸。但此处k-means的距离衡量不能用传统的欧氏距离，因为欧氏距离会使尺寸大的box产生的误差更大。我们真正需要的是有高IOU的先验，于是使用：

$$d(\text{box}, \text{centroid}) = 1 - \text{IOU}(\text{box}, \text{centroid})$$
运行各种$k$值的k-means，并画出与最接近的几何中心的平均IOU，选$k=5$作为复杂度与高recall的tradeoff。
![](_v_images/20190521161547308_8933.png =670x)
### 直接预测位置
使用anchor的第二个问题：预测边界框的$(x,y)$位置时带来的模型不稳定。在区域提出网络中，网络预测值$t_x$、$t_y$与$(x,y)$中心坐标计算如下：

$$
x = (t_x * w_a) - x_a\\
y = (t_y * h_a) - y_a
$$
$t_x$的预测值会导致anchor直接在x轴上不受限制地移动。随机初始化的参数会需要很长时间才能稳定以预测合理的偏移量。(个人理解是$t_x$应该是有定义域的，不能无限制取超出图像)

所以在本文中不再预测偏移量，而是采用YOLO的作风直接预测相对于网格的位置坐标。这样使用$\sigma$激活函数可以使该坐标限定在[0,1]上。

网络在feature map的每个单元上预测5个边界框，每个bbox预测5个坐标：$t_x$，$t_y$，$t_w$，$t_h$和$t_o$。如果单元相对于图像的左上角偏移了$(c_x, c_y)$，并且边界框先验的宽度和高度为$p_w$，$p_h$，那么预测对应：

$$
b_x = \sigma(t_x) + c_x \\
b_y = \sigma(t_y)  + c_y\\
b_w = p_w e^{t_w}\\
b_h = p_h e^{t_h}\\
Pr(\text{object}) * IOU(b, \text{object}) = \sigma(t_o)
$$
![](_v_images/20190521170424741_5309.png =660x)
### 细粒度特征Fine-Grained Features
Faster R-CNN及其变体使用多尺度（多级）来保证在较小物体上的识别率。本文采用一个passthrough层将前一层26\*26的feature加进来提取特征。

passthrough层通过 将相邻的特征堆叠到不同的通道 来把两个不同分辨率的层连接到一起。
### 多尺度训练
作者希望网络不局限于某一种尺寸大小的输入(因为是全卷积网络，确实可以调整网络输入)。于是在训练时，每隔10个批次YOLOv2会随机从{320,352，...，608}中选择一个新的图像尺寸大小进行训练。

这样的改变不仅使得网络更加鲁棒，而且为模型选择提供了新的方法。当输入尺寸小，分辨率低时，网络运行速度会变快；输入图像分辨率高时，网络运行速度会变慢但准确率会提高。这是一种简单的tradeoff。
## Faster
### Darknet-19
VGG是最常见的检测中特征提取器，但其运算量不可避免的大。本文使用基于GoogLeNet的自定义网络Darknet-19，虽然准确率有所下降，但速度大大提高。

大多使用3×3滤波器，并在每个池化步骤之后使通道数量加倍。按照Network in Network（NIN）的工作，使用全局平均池化做预测以及使用1×1滤波器来压缩两次3×3卷积之间的特征。一共有有19个卷积层和5个最大池化层，分类网络结构如下：
![](_v_images/20190528161956399_32005.png =670x)
在分类网络预训练中，使用随机梯度下降，初始学习率为0.1，学习率多项式衰减系数为4，权重衰减为0.0005，动量为0.9，在标准ImageNet 1000类分类数据集上训练网络160个迭代周期。使用标准的数据增强技巧，包括随机裁剪，旋转，色调，饱和度和曝光偏移。

检测网络在分类网络基础上删除了最后一个卷积层，加上了三个1024深度的3×3卷积层，这三个卷积层后都加了1×1卷积层，其深度是检测需要的输出数量。如在VOC数据集中，网络预测在每个网格单元预测五个Bounding Boxes，每个bbox预测5个坐标和20类，所以一共125个Filter。
![](_v_images/20190602204251793_3280.png =650x)
增加了从最后的3×3×512层到倒数第二层卷积层Passthough层来获取前面层的细粒度信息。
训练网络160个迭代周期，初始学习率为$10^{−3}$，在60个和90个迭代周期时将学习率除以10。使用0.0005的权重衰减和0.9的动量。使用数据增强，随机裁剪，色彩偏移等。

## Stronger
&emsp;&emsp;在训练时联合训练分classification和detection数据。在反向传播时根据该图片所属的不同数据集进行反向传播。eg：当网络遇到一个来自detection数据集的图片与标记信息，那么就把这些数据用完整的YOLO v2 loss功能反向传播这个图片。当网络遇到一个来自分类数据集的图片和分类标记信息，只用整个结构中分类部分的loss功能反向传播这个图片。
&emsp;&emsp;但是并不是这么简单一说就能做到的。classification和detection相比，种类信息更加细致，比如detection只会检测这是一只狗，但分类就会说这是只金毛。这两种不同的标注信息不能简单地叠加起来。比如原来classification标签1000个而detection标签100个，新的训练方式不能直接变成标签1100。因为通过softmax得到的结果是互斥的，而classification和detection的标签并不一定互斥，还有可能具有从属等关系。
### Hierarchical classification
ImageNet标签是从WordNet中提取的，这是一个构建概念及其相互关系的语言数据库。因为各个标签等级间的关系是多对多的，(真正正确的表达应该是有向图)我们使用分层树的概念来简化问题。在该树的建立过程中，尽量减少树的边，得到词意上的最小路径生成树。
![](_v_images/20190531111833037_10826.png =650x)
最终得到的标签结果是WordTree。对每个节点，预测的是条件概率，如上图所示，以cat节点为例，预测的就是：
```mathjax
$$
Pr(\text{tabby} | \text{cat}) \\ 
Pr(\text{Persian} | \text{cat}) \\ 
…\\
$$
```
用这样的节点条件概率，如果我们想得到某节点的绝对概率，只需要沿着边往上爬即可，如一个物体是猫的概率如下：
```mathjax
$$
Pr(\text{cat}) = Pr(\text{cat} | \text{animal}) *  
Pr(\text{animal} | \text{physical object})
$$
```
为了分类，我们令$Pr(\text{physical object}) = 1$。而在detection时，利用YOLO的目标检测器得到$Pr(\text{physical object})$的值。
预测时使用多个softmax，对某节点，给它分配的标签是该节点及其以上所有节点的标签。ImageNet1000的标签，在WordTree中增广成了1369个节点的向量。
![](_v_images/20190531144714850_6100.png =655x)
这样的操作虽然会使准确率下降，但是在对子种类不确定的时候还能预测标签为父种类（狗的置信度很高，而金毛的置信度很低）

检测时用预测器得到$Pr(\text{physical object})$，bbox坐标和概率树。遍历树，采用达到某阈值的最高置信度的路径，用该路径预测目标类。

这样使用WordTree还可以将不同的训练集之间
### 联合分类和检测
使用整个ImageNet和COCO数据集一起进行训练，一共有9418个节点。这样更有助于对没有标签的样本进行检测。而由于分类数据集远比检测数据集大，对COCO数据集进行了oversampling使得数据集大小比例为4：1(仍然是ImageNet更大)。
这一共9000个类的训练得到的网络即YOLO9000，由于输出太多采用3prior的设置，这样现在每个位置是3\*(4+1+9418)个输出。

当网络遇到检测数据集中的图片时则正常地反方向传播，当遇到分类数据集图片的时候，只使用分类的相应层的loss功能进行反向传播，假设IOU最少为0.3进行反向传播。